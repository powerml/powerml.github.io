{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>Llama can be installed using pip, the package manager for Python. To install Llama, open a command prompt and type:</p> <pre><code>pip install llama-llm\n</code></pre> <p>This will download and install the latest version of Llama and its dependencies.</p> <p>Check if your installation was done correctly, by importing the LLM engine in your python interpreter.</p> <pre><code>&gt;&gt; from llama import LLM\n</code></pre>"},{"location":"#setup-your-keys","title":"Setup your keys","text":"<p>Go to https://lamini.ai.  Log in to get your API key and purchase credits.</p> <p>Create <code>~/.powerml/configure_llama.yaml</code> and put a key in it.</p> <pre><code>production:\n    key: \"&lt;YOUR-KEY-HERE&gt;\"\n</code></pre> <p>See the Authentication page for more advanced options.</p>"},{"location":"#basic-test","title":"Basic test","text":"<p>Run the LLM engine with a basic test to see if installation and authentication were set up correctly.</p> <pre><code>from llama import LLM, Type, Context\nclass Test(Type):\ntest_string: str = Context(\"just a test\")\nllm = LLM(name=\"my_test\")\ntest = Test(test_string=\"testing 123\")\nllm(test, output_type=Test)\n</code></pre> <p>Now you're on your way to using the LLM engine on your specific use case!</p>"},{"location":"#try-an-example","title":"Try an example","text":"<ul> <li> <p>Marketing Copy Generation in Google Colab</p> </li> <li> <p>Tweet Generation in Google Colab</p> </li> </ul>"},{"location":"advanced/","title":"Advanced","text":"<p>Now that you've got the basics, it's time to power up your llama!</p>"},{"location":"advanced/#generate-varied-output","title":"Generate Varied Output","text":"<p>In the Llama walkthrough, you generated ad copy using the below:</p> <pre><code>class AdAspects(Type):\ntone: str = Context(\"tone of the marketing copy\")\nproduct_features: list = Context(\"product features to promote\")\naudience: str = Context(\"target audience for the message\")\nsubject: str = Context(\"subject or topic of the message\")\ngoal: str = Context(\"goal of this marketing campaign and message\")\nclass AdCopy(Type):\nh1: str = Context(\"google ad h1 tag\")\ntitle: str = Context(\"google ad title tag\")\nkeywords: list = Context(\"keywords for the search engine\")\naspects = AdAspects(\ntone=\"bold and bright, but not arrogant\",\nproduct_features=[\n'asian sauces and aromatics',\n'home-cooked seasonings and meal packs that can be easily cooked at home'\n],\naudience=\"suburban families\",\nsubject=\"delicious asian meals without going to a restaurant\",\ngoal=\"get suburban moms and dads to try buy their first omsom pack or free tasting kit\"\n)\nad_copy = llm(input=aspects, output_type=AdCopy)\n</code></pre> <p>Running this multiple times should assign the same value to <code>ad_copy</code>. But what if you want some variation? That's where <code>random</code> comes in! Instead, you can run the below multiple times to get different outputs:</p> <pre><code>ad_copy = llm(input=aspects, output_type=AdCopy, random=True)\n</code></pre>"},{"location":"advanced/#remove-duplicates","title":"Remove Duplicates","text":"<p><code>random</code> is great for inducing some variety, but what if you want to remove duplicates across copies? You can use <code>llm.sample</code> to produce a list of outputs that are all different from each other:</p> <pre><code>ad_copies = llm.sample(input=aspects, output_type=AdCopy, n=5)\n</code></pre> <p>You can also use <code>llm.sample</code> to ensure variation across attributes within a single copy, for example, making sure that the title and h1 are not the same:</p> <pre><code>ad_copy = llm.sample(input=aspects, output_type=AdCopy, n=1)[0]\n</code></pre>"},{"location":"advanced/#add-output-scores","title":"Add Output Scores","text":"<p>Now you can generate varied outputs, but how do you compare between them? You can do so by generating scores! First, update your output type to include the score you wish to generate:</p> <pre><code>class AdCopy(Type):\nh1: str = Context(\"google ad h1 tag\")\ntitle: str = Context(\"google ad title tag\")\nkeywords: list = Context(\"keywords for the search engine\")\nscore: float = Context(\"score for the ad copy\")\n</code></pre> <p>Then you can run your llm with the given score attribute to have it generate a confidence score for the output:</p> <pre><code>ad_copy = llm(input=aspects, output_type=AdCopy, random=True, score='score')\n</code></pre>"},{"location":"auth/","title":"Authentication","text":"<p>Welcome to Llama's exciting and secure world of authentication!</p> <p>To access Llama's services, you'll need an API key, which you can retrieve from your Llama account page. This API key is your secret, so be sure not to share it with anyone or expose it in any client-side code.</p> <p>To keep your API key safe, production requests should always be routed through your own backend server, where your API key can be securely loaded from an environment variable or key management service.</p> <p>Llama offers several ways to provide your API key:</p> <ul> <li>Config file</li> <li>Python API</li> <li>Authorization HTTP header</li> </ul>"},{"location":"auth/#config-file","title":"Config file","text":"<p>Ready to configure your Llama API key? It's easy-peasy! Create a secret config file and put your key in it to get started.</p> <p>First, navigate to your Llama account page to retrieve your unique API key. Remember to keep this key a secret and don't expose it in any client-side code or share it with others.</p> <p>Next, create a <code>~/.powerml/configure_llama.yaml</code> file and place your key in it, like so:</p> <pre><code>production:\n    key: \"&lt;YOUR-KEY-HERE&gt;\"\n</code></pre> <p>The best part? The Llama python package will automatically load your key from this config file for you, so you don't have to worry about it.</p> <p>If you're running Llama in a docker container, make sure to copy/mount this file inside the container.</p> <p>Configuring your Llama API key has never been so easy!</p>"},{"location":"auth/#python-api","title":"Python API","text":"<p>Feeling new to coding? Don't worry, we've got you covered with our awesome Python API.</p> <p>To get started, simply import our API client and initialize it with your API key:</p> <pre><code>from llama import LLM\nllm = LLM(\nname=\"marketing\",\nconfig={\n\"production\": {\n\"key\": \"&lt;YOUR-KEY-HERE&gt;\",\n}\n},\n)\n</code></pre> <p>After you've set up, it's time to flex your coding skills by making some epic calls to our API! You'll be a pro in no time!</p>"},{"location":"auth/#authorization-http-header","title":"Authorization HTTP header","text":"<p>All Llama REST API requests should include your API key in an Authorization HTTP header as follows:</p> <pre><code>Authorization: Bearer &lt;YOUR-KEY-HERE&gt;\n</code></pre>"},{"location":"auth/#accounts","title":"Accounts","text":""},{"location":"auth/#create-an-account","title":"Create an account","text":"<p>Yo, listen up! Creating an account with Llama is easy peasy, lemon squeezy. All you need is a Gmail address to sign in with Google single sign-on, and voila! We'll set up an account and token for you in no time.</p>"},{"location":"auth/#organizations","title":"Organizations","text":"<p>If you're running a large organization and need to manage multiple users on the same account, you should totally hit us up for an enterprise account. Just sign up and we'll get you sorted.</p>"},{"location":"auth/#google-colab","title":"Google Colab","text":"<p>We got your back when it comes to integrating with Google Colab, no sweat. Say goodbye to storing your API key in a notebook, and say hello to our effortless snippet for automatic login:</p> <pre><code># @title Setup: Authenticate with Google &amp; install the open-source [Llama library](https://pypi.org/project/llama-llm) to use LLMs easily\n%%capture\nfrom google.colab import auth\nimport requests\nimport os\nimport yaml\ndef authenticate_powerml():\nauth.authenticate_user()\ngcloud_token = !gcloud auth print-access-token\npowerml_token_response = requests.get('https://api.powerml.co/data_studio/auth/verify_gcloud_token?token=' + gcloud_token[0])\nreturn powerml_token_response.json()['token']\nproduction_token = authenticate_powerml()\n!pip install --upgrade --force-reinstall --ignore-installed llama-llm\nkeys_dir_path = '/root/.powerml'\nos.makedirs(keys_dir_path, exist_ok=True)\nkeys_file_path = keys_dir_path + '/configure_llama.yaml'\nwith open(keys_file_path, 'w') as f:\nyaml.dump(config, f, default_flow_style=False)\n</code></pre> <p>This code snippet stores your API key in the production_token variable and writes it to the config file for you. Easy, right? If you prefer, you can also pass your API key to the LLM object using the Python API.</p>"},{"location":"auth/#comparison-of-authentication-methods","title":"Comparison of authentication methods","text":"<p>Authentication methods can be a real head-scratcher, especially if you're new to the game. But fear not, my fellow memers! Llama has got you covered with three different authentication methods: config file, Python API, and Authorization HTTP header.</p>"},{"location":"auth/#config-file_1","title":"Config file","text":"<p>The config file is perfect for small-scale applications and personal projects, as it's easy to set up and configure. But beware, storing your API key in plain text on your machine is like putting your password on a post-it note. Keep that file to yourself, and don't let anyone else get their grubby little hands on it.</p>"},{"location":"auth/#python-api_1","title":"Python API","text":"<p>If you need flexibility and scalability for large-scale applications, the Python API method is the way to go. You can dynamically set your API key based on your app's requirements, and you can use it with various environments and applications. But still, don't go sharing your key with anyone or commit it to a version control system.</p>"},{"location":"auth/#authorization-http-header_1","title":"Authorization HTTP header","text":"<p>The Authorization HTTP header method is for the big boys, the ones with larger-scale apps and stringent security requirements. This method is secure because the API key isn't stored in plain text, but it requires additional implementation effort to set up correctly. Plus, managing and rotating API keys can be a real pain in the butt.</p> <p>In the end, you'll have to weigh the pros and cons of each method to determine which one is the best fit for your needs. Just remember, always keep your API key safe and secure like a secret meme stash.</p>"},{"location":"batching/","title":"Batching","text":"<p>Sometimes you'd like to submit a lot of input data for processing. Our batching interface is the best way to do so. This interface is not constrained by our API timeout. Results are made available at set intervals as portions of the job are completed.</p> <pre><code>llm = LLM(name=\"batch_example\")\n</code></pre>"},{"location":"batching/#submitting-a-job","title":"Submitting a job","text":"<p>Begin by submitting a job for processing. Upon submission, a job id will be returned.</p> <pre><code>job = llm.submit_job(self, input, output_type, *args, **kwargs)\n</code></pre>"},{"location":"batching/#checking-the-status-of-a-job","title":"Checking the status of a job","text":"<p>Use this utility to check the status of a job.</p> <pre><code>status = llm.check_job_status(self, job_id)\n</code></pre> <p>Possible statuses include</p> <pre><code>'NOT_SCHEDULED'\n'SCHEDULED'\n'RUNNING'\n'DONE'\n'ERRORED'\n'CANCELED'\n</code></pre> <ul> <li><code>NOT_SCHEDULED</code> - That job is not scheduled to run. Submit another job</li> <li><code>SCHEDULED</code> - That job is in our queue</li> <li><code>RUNNING</code> - Job is currently running</li> <li><code>DONE</code> - Job is completed</li> <li><code>ERRORED</code> - Job encountered an error. This job will be retried and rescheduled automatically.</li> <li><code>CANCELED</code> - Job was canceled. Submit another job.</li> </ul>"},{"location":"batching/#get-job-results","title":"Get job results","text":"<p>Once the job is completed, or once progress has been made, get those results with this utility.</p> <pre><code>results = llm.get_job_results(job_id, output_type)\n</code></pre>"},{"location":"batching/#cancelling-a-job","title":"Cancelling a job","text":"<p>At any point in the job execution, you can cancel the running job.</p> <pre><code>llm.cancel_job(job_id)\n</code></pre> <p>You'll still be able to get job results after canceling.</p>"},{"location":"error_handling/","title":"Error Handling","text":"<p>Eliminate errors with our comprehensive error handling documentation</p>"},{"location":"error_handling/#internal-server-500-error","title":"Internal Server 500 error","text":"<p>Internal server errors are errors on our server. These are our problems, so report these! They are usually caused by a misconfigured server, or an issue with the server's resources.</p> <p>Here are some ways in which you can resolve Internal Server 500 error:</p> <ol> <li> <p>Update the llama-llm python package to the most recent version: Make sure that you are using supported python version. Supported python version are 3.7 to 3.11.</p> <ol> <li> <p>Download the most recent python client from Llama python package.</p> </li> <li> <p>You can update your Python version by downloading the latest version from the Python website and running the installer.</p> </li> <li> <p>Alternatively, you can update Python using a package manager such as Homebrew (for macOS) or apt-get (for Linux)</p> </li> </ol> </li> <li> <p>Review the script: This error could arise due to mismatch in defined Type format. Make sure that the input and output types are defined in following format:</p> </li> </ol> <p><pre><code>    [user_defined_type_name] : [python_data_type] = Context(\"\")\n</code></pre> Be sure to include the Context. This helps the LLM understand your types in natural language. Example: <pre><code>from llama import Type, Context\nclass AdAspects(Type):\ntone: str = Context(\"tone of the marketing copy\")\nproduct_features: list = Context(\"product features to promote\")\naudience: str = Context(\"target audience for the message\")\nsubject: str = Context(\"subject or topic of the message\")\ngoal: str = Context(\"goal of this marketing campaign and message\")\n</code></pre></p>"},{"location":"error_handling/#timeout-error","title":"Timeout error","text":"<p>These errors can occur if a request takes too long to process or if the server is unable to fulfill a request due to internal issues or resource constraints. Resource constraints can occur if the server has too little available memory, disk space, or other resources to process a request. Usually request to lamini api expires after 240 secs. You can try following solutions:</p> <ol> <li> <p>Using PowerML batching interface, you can learn more about it here</p> </li> <li> <p>If above solution doesn't resolve timeout error then try reruning the program</p> </li> </ol>"},{"location":"error_handling/#authentication-error","title":"Authentication error","text":"<p>Authentication errors occur when a user attempts to access a secured resource but fails to provide the correct credentials. This can occur if the user has entered their credentials incorrectly, if the credentials are out of date, or if the credentials are not authorized to access the resource. Authentication errors can also occur if the authentication system itself has malfunctioned or has been compromised. You can resolve it by setting correct authentication token. More detailed documentation can be found here</p>"},{"location":"example/","title":"Walkthrough Example","text":""},{"location":"example/#import-llama-and-initialize-an-llm-engine","title":"Import Llama and initialize an LLM engine","text":"<pre><code>from llama import LLM\nllm = LLM(name=\"marketing\")\n</code></pre>"},{"location":"example/#define-the-llm-interface","title":"Define the LLM interface","text":"<p>Define the input and output types. Be sure to include the <code>Context</code>. This helps the LLM understand your types in natural language.</p> <pre><code>from llama import Type, Context\nclass AdAspects(Type):\ntone: str = Context(\"tone of the marketing copy\")\nproduct_features: list = Context(\"product features to promote\")\naudience: str = Context(\"target audience for the message\")\nsubject: str = Context(\"subject or topic of the message\")\ngoal: str = Context(\"goal of this marketing campaign and message\")\nclass AdCopy(Type):\ntitle: str = Context(\"google ad title tag\")\ndescription: str = Context(\"google ad description\")\nkeywords: list = Context(\"keywords for the search engine\")\n</code></pre>"},{"location":"example/#run-the-llm","title":"Run the LLM","text":""},{"location":"example/#generate-ad-copy-from-different-aspects-you-want","title":"Generate ad copy from different aspects you want","text":"<pre><code>aspects = AdAspects(\ntone=\"bold and bright, but not arrogant\",\nproduct_features=[\n'asian sauces and aromatics',\n'home-cooked seasonings and meal packs that can be easily cooked at home'\n],\naudience=\"suburban families\",\nsubject=\"delicious asian meals without going to a restaurant\",\ngoal=\"get suburban moms and dads to try buy their first omsom pack or free tasting kit\"\n)\nad_copy = llm(input=aspects, output_type=AdCopy)\nprint(f\"Ad copy: {ad_copy}\")\n</code></pre> <p>Output: <pre><code>&gt; title='Delicious Asian Meals Without Going to a Restaurant | Omsom'\ndescription=\"Try Omsom's delicious Asian sauces, aromatics, and home-cooked seasonings and meal packs. Easily cook delicious meals at home for your family.\"\nkeywords=[\n'Asian sauces',\n    'Aromatics',\n    'Home-cooked seasonings',\n    'Meal packs',\n    'Delicious meals',\n    'Suburban families',\n    'Omsom'\n]\n</code></pre></p>"},{"location":"example/#extract-ad-aspects-from-the-copy-you-already-have","title":"Extract ad aspects from the copy you already have","text":"<pre><code>ad_copy = AdCopy(\ntitle=\"Omsom | Proud, loud Asian home cooking\",\ndescription=\"An Omsom starter is a pantry shortcut for a specific Asian dish, combining all the sauces, aromatics, and seasonings you need.\",\nkeywords=[\n\"asian sauces\",\n\"asian food\",\n\"home-cooked asian meals\",\n\"home-cooked seasonings\",\n\"at home\"\n]\n)\nad_aspects = llm(input=ad_copy, output_type=AdAspects)\nprint(f\"Ad aspects: {ad_aspects}\")\n</code></pre> <p>Output: <pre><code>&gt; tone='Exciting and proud'\nproduct_features=[\n'Ready-made sauces and seasonings',\n    'Variety of Asian dishes',\n    'Easy to use'\n]\naudience='Home cooks looking for an easy way to make Asian dishes'\nsubject='Proud, loud Asian home cooking'\ngoal=\"To encourage home cooks to try out Asian dishes with the help of Omsom's ready-made sauces and seasonings.\"\n</code></pre></p>"},{"location":"example/#improve-the-llm-with-feedback","title":"Improve the LLM with feedback","text":"<pre><code>llm.improve(on=\"keywords\", to=\"cite specific {product_features}\")\nad_copy = llm(input=aspects, output_type=AdCopy)\nprint(f\"Ad copy after improving: {ad_copy}\")\n</code></pre> <p>Output: <pre><code>&gt; Ad copy after improving:\n  title='Delicious Asian Meals From Omsom \ud83c\udf71'\ndescription=\"Try Omsom's delicious Asian sauces, aromatics, and home-cooked seasonings and meal packs. Easily cook delicious meals at home for your family. \ud83c\udf72\"\nkeywords=[\n'Asian sauces',\n    'Aromatics',\n    'Home-cooked seasonings',\n    'Meal packs',\n    'Delicious meals',\n    'Suburban families',\n    'Omsom',\n    'Emojis',\n    'Gluten-free',\n    'Vegan-friendly',\n    'Low-sodium',\n    'No-MSG'\n]\n</code></pre></p>"},{"location":"example/#train-the-llm-on-your-data","title":"Train the LLM on your data","text":"<pre><code># In the format of [[AdAspects, AdCopy], [AdAspects, AdCopy], ...]\ndata = get_my_marketing_data()\nllm.add_data(data)\nad_copy = llm(input=aspects, output_type=AdCopy)\nprint(f\"Ad copy after adding data: {ad_copy}\")\n</code></pre> Code for <code>get_my_marketing_data()</code> <pre><code>def get_my_marketing_data():\nreturn [\n[\nAdAspects(\ntone='Exciting and modern',\nproduct_features=['Made from oak', 'Variety of meats and cheeses', 'Perfect for entertaining'],\naudience='Home chefs and entertainers',\nsubject='Elevate your entertaining with charcuterie boards',\ngoal='To showcase the versatility and convenience of charcuterie boards as an entertaining option.',\n),\nAdCopy(\ntitle='\ud83e\uddc0 Charcuterie Boards Made from Oak | Boardsy',\ndescription='Get the perfect charcuterie board made from oak for your next gathering. Our key product feature is charcuterie boards made from oak. Shop now with Brand Name.',\nkeywords=['charcuterie boards', 'oak', 'key product feature'],\n),\n],\n[\nAdAspects(\ntone='Celebratory',\nproduct_features=['Anniversary messages', 'Customizable messages', 'Personalized messages'],\naudience='Couples celebrating anniversaries',\nsubject='Celebrate Your Anniversary with a Special Message',\ngoal='To encourage couples to celebrate their anniversaries with a special message.',\n),\nAdCopy(\ntitle='\ud83c\udf89 Anniversary Messages - Key Product Feature \ud83c\udf89 | Hollamark',\ndescription='Celebrate your special day with our key product feature - anniversary messages. Send heartfelt wishes to your loved ones with our unique and personalized messages from Brand Name.',\nkeywords=['anniversary messages', 'key product feature', 'personalized messages', 'heartfelt wishes'],\n),\n],\n[\nAdAspects(\ntone='Exciting and enthusiastic',\nproduct_features=['Unique flavor combinations', 'All-natural ingredients', 'Hand-crafted in small batches'],\naudience='Home cooks and foodies',\nsubject='Unlocking the flavors of the world',\ngoal='To introduce customers to the unique flavor combinations of artisanal spice blends and encourage them to explore new culinary experiences.',\n),\nAdCopy(\ntitle='\ud83c\udf36\ufe0f Artisanal Spices - Spice Blends Key Product Feature \ud83c\udf36\ufe0f | Shop Now with Artisanal Spices!',\ndescription='Discover the unique flavors of artisanal spice blends with our key product feature. Shop now with Artisanal Spices! \ud83d\uded2',\nkeywords=['artisanal spices', 'artisanal spice blends', 'key product feature', 'unique flavors', 'shop now', 'emojis'],\n),\n],\n[\nAdAspects(\ntone='Exciting and energetic',\nproduct_features=['Comfort', 'Durability', 'Breathability', 'Stylish design'],\naudience='Active women',\nsubject='Look and feel your best with yoga pants',\ngoal='To promote the benefits of yoga pants and encourage active women to purchase them.',\n),\nAdCopy(\ntitle='\ud83e\uddd8\u200d\u2640\ufe0f Zennn Yoga Pants - Key Product Feature \ud83e\uddd8\u200d\u2640\ufe0f',\ndescription='Get the perfect fit and feel with Zennn\\'s key product feature - yoga pants. Shop now for the best selection and prices.',\nkeywords=['yoga pants', 'key product feature', 'perfect fit', 'best selection', 'best prices'],\n),\n],\n[\nAdAspects(\ntone='Exciting and informative',\nproduct_features=['Perfect recipes for keto dieters', 'Easy to follow instructions', 'Nutritional information for each recipe'],\naudience='Keto dieters looking for meal ideas',\nsubject='Delicious Keto Recipes',\ngoal='To promote the key product feature of perfect recipes, keto and encourage keto dieters to try the recipes.',\n),\nAdCopy(\ntitle='\ud83c\udf7d Perfect Recipes for Keto Dieters - Key Product Feature \ud83e\udd57 | Saladmania',\ndescription='Get the perfect recipes for your keto diet with our key product feature. Enjoy delicious meals and stay on track with your diet. \ud83c\udf7d | Brand Name',\nkeywords=['perfect recipes', 'keto diet', 'key product feature', 'delicious meals'],\n),\n],\n[\nAdAspects(\ntone='Fun and exciting',\nproduct_features=['Unique flavors', 'Variety of toppings', 'Customizable options'],\naudience='Young adults and families',\nsubject='Enjoy delicious ice cream at the microcreamery',\ngoal='To increase awareness of the microcreamery and encourage customers to visit and try the unique flavors and toppings.',\n),\nAdCopy(\ntitle='\ud83c\udf66 Delicious Ice Cream from Microcreamery - Key Product Feature \ud83c\udf66',\ndescription='Enjoy delicious ice cream from Microcreamery, with a key product feature that sets it apart from the competition. \ud83c\udf66',\nkeywords=['ice cream, microcreamery, key product feature, delicious, emojis, Microcreamery, brand name'],\n),\n],\n[\nAdAspects(\ntone='Exciting and Innovative',\nproduct_features=['Easy to use interface', 'Comprehensive data tracking', 'Automated reporting', 'Customizable settings'],\naudience='Ohio-based software developers',\nsubject='Unlocking the Potential of Ohio Statewide Software Leagues',\ngoal='To showcase the features of Ohio statewide software leagues and demonstrate how they can help software developers maximize their potential.',\n),\nAdCopy(\ntitle='\ud83c\udfae Ohio Statewide Software Leagues - Key Product Feature | Ohio Statewide',\ndescription='Get the most out of your software with Ohio Statewide Software Leagues. Our key product feature is designed to help you \ud83d\ude80 maximize your software\\'s potential. | Ohio Statewide',\nkeywords=['Ohio Statewide Software Leagues', 'Key Product Feature', 'Software Leagues', 'Maximize Software Potential', 'Ohio Statewide'],\n),\n],\n[\nAdAspects(\ntone='Inspirational',\nproduct_features=['Variety of scripts', 'Professional guidance', 'Access to industry professionals'],\naudience='Actors and actresses',\nsubject='Unlocking Your Potential as an Actor or Actress',\ngoal='To inspire and motivate actors and actresses to reach their full potential through practice theater scripts.',\n),\nAdCopy(\ntitle='\ud83c\udfad Practice Theater Scripts for Actors &amp; Actresses by Brand Name \ud83c\udfad',\ndescription='Get the best \ud83c\udfad practice theater scripts for actors and actresses from Brand Name. Improve your performance with our selection of scripts.',\nkeywords=['practice theater scripts', 'theater scripts for actors', 'theater scripts for actresses', 'improve performance', 'theater scripts'],\n),\n],\n[\nAdAspects(\ntone='Inspirational',\nproduct_features=['High intensity interval training', 'Strength training', 'Cardio workouts', 'Nutrition advice'],\naudience='Men aged 18-35',\nsubject='Get Fit and Healthy with Mens Health Workouts',\ngoal='To inspire men to take control of their health and fitness through mens health workouts.',\n),\nAdCopy(\ntitle='\ud83d\udcaa Mens Health Workouts - Get Fit Now! | Mens Health',\ndescription='Get fit now with Mens Health Workouts. Get the best results with our tailored programs and expert advice. | Mens Health',\nkeywords=['mens health workouts', 'fitness', 'exercise', 'get fit', 'mens health'],\n),\n],\n[\nAdAspects(\ntone='Informative and helpful',\nproduct_features=['Easy to use', 'Accurate calculations', 'Comprehensive loan comparison', 'Customizable repayment plans'],\naudience='College students and recent graduates',\nsubject='Student loan calculators',\ngoal='To inform college students and recent graduates of the benefits of using student loan calculators to compare and customize their loan repayment plans.',\n),\nAdCopy(\ntitle='\ud83e\udd13 ABC Financial - Get the Best Student Loan Calculators \ud83e\udd13',\ndescription='Get the best student loan calculators from ABC Financial to help you manage your finances. Our key product feature helps you make the right decisions. \ud83e\udd13',\nkeywords=['student loan calculators', 'ABC Financial', 'key product feature', 'student loan calculator', 'loan calculator', 'student loan repayment calculator', 'student loan repayment'],\n),\n],\n]\n</code></pre> <p>Output: <pre><code>&gt; Ad copy after adding data:\n  title='\ud83e\udd62 Delicious Asian Meals at Home with Omsom - Key Product Feature \ud83e\udd62 | Omsom'\ndescription=\"Get delicious Asian meals at home with Omsom's key product feature. Enjoy the flavors of Asia without going to a restaurant. \ud83e\udd62 | Omsom\"\nkeywords=[\n'Asian meals',\n    'Key product feature',\n    'Asian sauces',\n    'Aromatics',\n    'Home-cooked seasonings',\n    'Meal packs',\n    'Omsom',\n  ]\n</code></pre></p>"},{"location":"example/#more-advanced-training","title":"More advanced training","text":"<p>Coming soon to the docs :)</p>"},{"location":"example/#additional-examples-in-public-notebooks","title":"Additional examples in public notebooks","text":"<ul> <li> <p>Marketing Copy Generation in Google Colab</p> </li> <li> <p>Tweet Generation in Google Colab</p> </li> </ul>"},{"location":"python_library/","title":"Python library","text":"<p>Llama is a Python package designed to build Language Learning Models (LLMs) for natural language processing tasks. It provides an engine for creating and running your own LLMs. With Llama, you can train language models on large text corpora and improve them following your guidelines, which can then be used for generating and extracting text.</p>"},{"location":"python_library/#input-and-output-types","title":"Input and output types","text":"<p>First, you want to construct some data types: (1) input types as arguments into the LLM and (2) output types as return values from the LLM.</p> <p>You can use the <code>Type</code> and <code>Context</code> classes in the library create them.</p> <p>For example, you can create an <code>Animal</code> type as follows:</p> <pre><code>from llama import Type, Context\nclass Animal(Type):\nname: str = Context(\"name of the animal\")\nn_legs: int = Context(\"number of legs that animal has\")\nllama_animal = Animal(name=\"Larry\", n_legs=4)\n</code></pre> <p>Each <code>Type</code> requires at least one attribute, such as <code>name</code> and <code>n_legs</code> here. They can be anything you would like. Be sure to add a <code>Context</code> field to each attribute, with a natural language description of the attribute. That is required to tell the model what you mean by each attribute.</p>"},{"location":"python_library/#running-the-llm","title":"Running the LLM","text":"<p>Next, you want to instantiate your LLM engine with <code>LLM</code>.</p> <pre><code>llm = LLM(name=\"animal_stories\")\n# If you want to use a different base model or add your config options here\nllm = LLM(\nname=\"my_llm_name\",\nmodel_name=\"chat-gpt\",\nconfig={\n\"production\": {\n\"key\": \"&lt;API-KEY-HERE&gt;\",\n}\n},\n)\n</code></pre> <p>Now, you can now run your LLM.</p> <pre><code># Define an output type\nclass Story(Type):\nstory: str = Context(\"Story of an animal\")\nllama_animal = Animal(name=\"Larry\", n_legs=4)\nllama_story = llm(llama_animal, output_type=Story)\n</code></pre>"},{"location":"python_library/#adding-data","title":"Adding data","text":"<p>You have data on different inputs and outputs, and in some cases, you have pairs of inputs and outputs that you want the LLM to model after.</p> <p>Getting data of good inputs: <pre><code>llama_animal = Animal(name=\"Larry\", n_legs=4)\ncentipede_animal = Animal(name=\"Cici\", n_legs=100)\nmy_data = [llama_animal, centipede_animal]\n</code></pre></p> <p>Getting data of a good input-output pair: <pre><code>dog_animal = Animal(name=\"Nacho\", n_legs=4)\ndog_speed = Story(story=\"There once was a cute doggo named Nacho. She was a golden retriever who liked to run. All four of her paws were adorable.\")\nmy_data.append([dog_animal, dog_speed])\n</code></pre></p> <p>Now add all that data to your LLM: <pre><code>llm.add_data(my_data)\n</code></pre></p> <p>With the same call to the LLM engine, it should now produce a story that is more aligned with your data. <pre><code>llama_story = llm(llama_animal, output_type=Story)\n</code></pre></p>"},{"location":"python_library/#improving-with-criteria","title":"Improving with criteria","text":"<p>Now that you've added data, you want to improve the model's outputs further. Another way to do that is to supply <code>improve</code> statements on different attributes of a model's output type to improve on. You can use natural language to tell the model how it should improve.</p> <pre><code>llm.improve(on=\"story\", to=\"specify the number of legs in a subtle way\")\n</code></pre>"},{"location":"python_library/#full-example","title":"Full example","text":"<p>Start with data.</p> <pre><code>class Animal(Type):\nname: str = Context(\"name of the animal\")\nn_legs: int = Context(\"number of legs that animal has\")\nclass Speed(Type):\nspeed: float = Context(\"how fast something can run\")\nllama_animal = Animal(name=\"Larry\", n_legs=4)\ncentipede_animal = Animal(name=\"Cici\", n_legs=100)\nmy_data = [llama_animal, centipede_animal]\ndog_animal = Animal(name=\"Nacho\", n_legs=4)\ndog_speed = Story(story=\"There once was a cute doggo named Nacho. She was a golden retriever who liked to run. All four of her paws were adorable.\")\nmy_data.append([dog_animal, dog_speed])\n</code></pre> <p>Instantiate the LLM engine, add data, add improvements (as many as you like), and run the LLM engine.</p> <pre><code>llm = LLM(name=\"animal_stories\")\nllm.add_data(my_data)\nllm.improve(on=\"story\", to=\"specify the number of legs in a subtle way\")\nstory = llm(llama_animal, output_type=Story)\n</code></pre> <p>A common workflow is to run the LLM engine and see issues in the LLM outputs, then add an improve statement and run the LLM engine again.</p>"},{"location":"LLM/__call__/","title":"llama.LLM.__call__","text":"<p>Runs the instantiated LLM engine.</p> <pre><code>llm(input, output_type, input_type)\n</code></pre>"},{"location":"LLM/__call__/#parameters","title":"Parameters","text":"<ul> <li>input: <code>&lt;class llama.Type&gt;</code> - name of the LLM engine instance</li> <li>output_type: <code>&lt;class llama.Type&gt;</code> - the type of the output</li> <li>input_type: <code>&lt;class llama.Type&gt;</code> (Optional) - the type of the input (also inferred by the engine with <code>input</code>, so it is optional)</li> </ul>"},{"location":"LLM/__call__/#returns","title":"Returns","text":"<p>output: <code>&lt;class 'llama.Type&gt;</code> - output of the LLM, based on <code>input</code>, in the type specified by <code>output_type</code></p>"},{"location":"LLM/__call__/#example","title":"Example","text":"<pre><code>llm = LLM(name=\"my_llm_name\")\nmy_output = llm(my_input, output_type=MyOutputType)\n</code></pre>"},{"location":"LLM/__init__/","title":"llama.LLM.__init__","text":"<p>Class that instantiates the LLM engine.</p> <pre><code>LLM(name, model_name, config)\n</code></pre>"},{"location":"LLM/__init__/#attributes","title":"Attributes","text":"<ul> <li>name: <code>str</code> - name of the LLM engine instance</li> <li>model_name: <code>str</code> (Optional) - name of the base model, defaults to OpenAI's <code>text-davinci-003</code>.</li> <li>config: <code>dict</code> (Optional) - auth-related parameters, e.g. token</li> </ul>"},{"location":"LLM/__init__/#example","title":"Example","text":"<pre><code>llm = LLM(name=\"my_llm_name\")\n# With optional parameters\nllm = LLM(\nname=\"my_llm_name\",\nmodel_name=\"chat-gpt\",\nconfig={\"token\": \"my_token\"}\n)\n</code></pre>"},{"location":"LLM/add_data/","title":"llama.LLM.add_data","text":"<p>Adds data to the LLM.</p> <pre><code>llm.add_data(my_data)\nllm.add_data([my_data, my_other_data, ...]\nllm.add_data([[input_data, output_data], [more_input_data, more_output_data], ...])\n</code></pre>"},{"location":"LLM/add_data/#parameters","title":"Parameters","text":"<ul> <li>data: <code>&lt;class 'llama.types.type.Type'&gt;</code> or <code>List[&lt;class 'llama.types.type.Type'&gt;]</code> or <code>List[List[&lt;class 'llama.types.type.Type'&gt;]]</code> - data structured as <code>Type</code>'s, grouped in lists because they're related to each other, and added in bulk through lists. 1-100k data elements are recommended at a time.</li> </ul>"},{"location":"LLM/add_data/#example","title":"Example","text":"<pre><code>llama_animal = Animal(name=\"Larry\", n_legs=4)\ncentipede_animal = Animal(name=\"Cici\", n_legs=100)\nmy_data = [llama_animal, centipede_animal]\ndog_animal = Animal(name=\"Sally\", n_legs=4)\ndog_speed = Speed(speed=30.0)\nmy_data.append([dog_animal, dog_speed])\nllm.add_data(my_data)\n</code></pre>"},{"location":"LLM/cancel_job/","title":"llama.LLM.cancel_job","text":"<p>Cancel a job</p> <pre><code>LLM.cancel_job(job_id)\n</code></pre>"},{"location":"LLM/cancel_job/#parameters","title":"Parameters","text":"<ul> <li>job_id: <code>str</code> - unique job id</li> </ul>"},{"location":"LLM/cancel_job/#returns","title":"Returns","text":"<p>None</p>"},{"location":"LLM/check_job_status/","title":"llama.LLM.check_job_status","text":"<p>Check the status of a job</p> <pre><code>status = LLM.check_job_status(job_id)\n</code></pre>"},{"location":"LLM/check_job_status/#parameters","title":"Parameters","text":"<ul> <li>job_id: <code>str</code> - unique job id</li> </ul>"},{"location":"LLM/check_job_status/#returns","title":"Returns","text":"<p>status: <code>dict</code> - a dictionary with status information</p> <p>Scheduled jobs will have the following returned</p> <pre><code>{'status': 'SCHEDULED'}\n</code></pre> <p>Just starting jobs will have the following format returned</p> <pre><code>{\n    'status': 'RUNNING',\n    'progress': 'Starting Run.',\n    'starttime': 1680724301.7032173\n}\n</code></pre> <p>While jobs that have made some progress will have the following format returned</p> <pre><code>{\n    'status': 'RUNNING',\n    'progress': 'Progress: 1 iterations out of 2.',\n    'starttime': 1680724301.7032173,\n    'time_elapsed': '8.602318525314331',\n    'average_runtime': '8.602318525314331',\n    'estimated_total_time': '16.602318525314331',\n    'estimated_time_remaining': '8.602318525314331'\n}\n</code></pre> <p>Completed jobs will have the following format returned</p> <pre><code>{\n    'status': 'DONE',\n    'progress': 'Progress: 3 iterations out of 3.',\n    'starttime': 1680724434.2409794,\n    'time_elapsed': '20.019465446472168',\n    'average_runtime': '6.673155148824056',\n    'estimated_total_time': '20.019465446472168',\n    'estimated_time_remaining': '0.0'\n}\n</code></pre>"},{"location":"LLM/check_job_status/#statuses","title":"Statuses","text":"<p>Possible statuses include</p> <pre><code>'NOT_SCHEDULED'\n'SCHEDULED'\n'RUNNING'\n'DONE'\n'ERRORED'\n'CANCELED'\n</code></pre>"},{"location":"LLM/check_job_status/#running-information","title":"Running Information","text":"<ul> <li><code>progress</code> - A description of the progress made in terms of iterations. Each iteration represents an equal subset of the data.</li> <li><code>starttime</code> - Job starttime, unixtime in seconds</li> <li><code>time_elapsed</code> - Amount of time elapsed since the start time</li> <li><code>average_runtime</code> - Average runtime per iteration in seconds thus far</li> <li><code>estimated_total_time</code> - Estimated total runtime based on average runtime in seconds</li> <li><code>estimated_time_remaining</code> - Estimated total time remaining based on average runtime in seconds</li> </ul>"},{"location":"LLM/get_job_results/","title":"llama.LLM.get_job_results","text":"<p>Get the job results</p> <pre><code>status = LLM.get_job_results(job_id, output_type)\n</code></pre>"},{"location":"LLM/get_job_results/#parameters","title":"Parameters","text":"<ul> <li>job_id: <code>str</code> - unique job id</li> <li>output_type: <code>&lt;class 'llama.types.type.Type'&gt;</code> - the desired data type of returned ouput</li> </ul>"},{"location":"LLM/get_job_results/#returns","title":"Returns","text":"<p>output: <code>list</code> - a list of Type objects</p>"},{"location":"LLM/improve/","title":"llama.LLM.improve","text":"<p>Improves the LLM to produce better output, following your natural language criteria.</p> <pre><code>llm.improve(on, to)\n</code></pre>"},{"location":"LLM/improve/#parameters","title":"Parameters","text":"<ul> <li>on: <code>str</code> - attribute in an output's <code>Type</code> to improve on</li> <li>to: <code>str</code> - natural language description of how to improve the LLM</li> </ul>"},{"location":"LLM/improve/#example","title":"Example","text":"<pre><code>llm.improve(on=\"speed\", to=\"give the average speed, not the max speed\")\n</code></pre>"},{"location":"LLM/sample/","title":"LLM.sample","text":"<p>Generate a list of unique outputs.</p> <pre><code>job = LLM.sample(input, output_type, n, *args, **kwargs)\n</code></pre>"},{"location":"LLM/sample/#parameters","title":"Parameters","text":"<ul> <li>input: <code>Union[Type, List[Type]]</code> - input data</li> <li>output_type: <code>&lt;class 'llama.types.type.Type'&gt;</code> - the desired data type of returned ouput</li> <li>n: <code>int</code> - number of samples to generate</li> </ul>"},{"location":"LLM/sample/#returns","title":"Returns","text":"<p>output: <code>list</code> - a list of Type objects</p>"},{"location":"LLM/submit_job/","title":"llama.LLM.submit_job","text":"<p>Submits a job for processing.</p> <pre><code>job = LLM.submit_job(input, output_type, *args, **kwargs)\n</code></pre>"},{"location":"LLM/submit_job/#parameters","title":"Parameters","text":"<ul> <li>input: <code>Union[Type, List[Type]]</code> - input data</li> <li>output_type: <code>&lt;class 'llama.types.type.Type'&gt;</code> - the desired data type of returned ouput</li> </ul>"},{"location":"LLM/submit_job/#returns","title":"Returns","text":"<p>job: <code>dict</code> - a dictionary with <code>status</code> and <code>job_id</code> string fields</p> <pre><code>{\n    'status': 'SCHEDULED',\n    'job_id': '-1579724389638199208'\n}\n</code></pre>"}]}